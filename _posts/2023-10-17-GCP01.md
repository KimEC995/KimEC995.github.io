---
title: GCP 설정하기(Spark 설치하기)
author: Kimec995
date: 2023-10-17 14:00:00 +09:00
categories: [GCP, Setup]
tags: [GCP,Cloud, NetWork, Setup]
pin: true
math: true
mermaid: true
image: 
  path: /assets/img/postimg/GCP/GCP_icon_00.png
  alt: GCP 설정하기
---

Google Cloud Platform의 VM 머신에 Spark를 설치하는 과정을 정리한다.

> 23.10.17 작성

## 0. 프로젝트 생성

만약 새롭게 시작한다면 프로젝트 생성부터 시작한다.

![image.png](\assets\img\postimg\GCP\GCP_01.png)

프로젝트 탭을 열어 새 프로젝트를 생성한 후

![image.png](\assets\img\postimg\GCP\GCP_02.png)

이름을 지어주고 `만들기`

단, 이름은 이후 수정이 불가하다.

이후 과정은 새로운 프로젝트 내부에서 사용한다.

만약 삭제를 원한다면 `프로젝트 설정` => `프로젝트 종료` => `프로젝트 명 작성` 한다. 프로젝트는 종료되고 삭제는 한 달후 진행된다.

## 1. 인스턴스 생성과 보안


### 1. 인스턴스 생성
원하는 프로젝트로 이동 후 인스턴스를 생성한다.

![image.png](\assets\img\postimg\GCP\GCP_03.png)

`VM인스턴스`에 진입하면 인스턴스 리스트가 나오는데, `인스턴스 만들기` 버튼을 눌러 생성한다.

![image.png](\assets\img\postimg\GCP\GCP_04.png)

![image.png](\assets\img\postimg\GCP\GCP_05.png)

옆에 요금창이 나온다. 무료 기간이면 무료로 사용되지만 기간이 지나면 해당 금액으로 부과(사용하면)된다. 리소스에 따라 부과 요금이 달라지니 주머니 사정에 잘 맞게 설정하자.

- **이름**: 인스턴스의 이름. 헷갈리지 않게 잘 작성하자.

- **리전**: 데이터센터의 위치. 한국이면 한국 리전이 가장 좋으나 백업용으로 타 국가에 설정하는 경우도 많다. ex) 한국의 지진 등 재난상황을 대비해 미국에 설정

- **영역**: 리전 내의 독립된 영역. 같은 데이터 센터에 위치해도 전력배치 등 물리적 요소로 분리되어있음.

- **머신 구성**: 하드웨어 리소스. 원하는 리소스를 선택하자. 만약 하드웨어 지식이 없다면 기본을 선택하자.

- **부팅 디스크**: OS 선택.

- **방화벽**: 네트워크 트래픽 허용 범위를 구성함. 요즘은 HTTPS를 사용하는데 가끔 HTTP도 사용함.

![image.png](\assets\img\postimg\GCP\GCP_06.png)


까지만 선택하면 일반 세팅은 완료되었다. `만들기` 버튼을 누르면 생성된다.

### 참고.

가상 PC이기 때문에 로컬의 전원을 내려도 인스턴스는 활성화 되어있다. 요금폭탄이 싫다면 사용 중지하자.

![image.png](\assets\img\postimg\GCP\GCP_09.png)

정지가 아니라 `중지`다.


### 2. 방화벽 설정

![image.png](\assets\img\postimg\GCP\GCP_07.png)

![image.png](\assets\img\postimg\GCP\GCP_08.png)

`방화벽 규칙 만들기`

- **대상**: 접속 환경 필터. `네트워크의 모든 인스턴스` 를 선택하면 네트워크에서 접속하는 인스턴스를 모두 허용함

- **소스 IPv4 범위**: IPv4의 이름을 작성. 모든 IPv4를 허용하면 `0.0.0.0/0`

- **프로토콜 및 포트**: 접속 프로토콜과 포트를 지정. 모두 선택하거나 지정만 가능

이렇게 환경에 맞게 나머지 설정

`만들기`

## 2. SSH - 브라우저

인스턴스를 SSH 접속하자. 다른 방법을 이용해도 좋지만 브라우저를 이용해 접속 할 것이다.

![image.png](\assets\img\postimg\GCP\GCP_10.png)

누르면 체크가 나오는데 체크하고 계정 연결.

![image.png](\assets\img\postimg\GCP\GCP_11.png)

터미널 화면이 나온다!!

## 3. 미니콘다 설치

```bash
cd ../../opt
```
로 디렉토리 이동 후

[미니콘다](https://docs.conda.io/projects/miniconda/en/latest/)

Quick install 명령어를 터미널에서 실행

![image.png](\assets\img\postimg\GCP\GCP_12.png)

그리고 재부팅하면

![image.png](\assets\img\postimg\GCP\GCP_13.png)

미니콘다가 설치된걸 확인 가능하다.

```bash
sudo apt update
```
로 apt 업데이트.

설치 가능한 패키지들을 최신화 한다.(설치아님)

## 4. 프로그램 설치

### 1. Java 설치

**opt 디렉토리로 이동 후**
```bash
cd ../../opt
```

**`openjdk-8` 을 설치**
```bash
sudo apt install openjdk-8-jdk
```

**Vi 편집**
```bash
vi ~/.bashrc
```

**환경변수 설정**
(다행히 GCP의 터미널은 Ctr 기능키가 먹힌다)
```bash
# JAVA ENV SET
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH
export CLASS_PATH=$JAVA_HOME/lib:$CLASS_PATH
```

**수정된 파일 실행**
```bash
source ~/.bashrc
```

### 2. Scala 설치

```bash
sudo apt-get install scala -y
```

**vi 편집기 실행**
```bash
vi ~/.bashrc
```

**환경변수 설정**
```bash
# SCALA ENV SET
export SCALA_HOME=/usr/bin/scala
export PATH=$SCALA_HOME/bin:$PATH
```

**수정된 파일 실행**
```bash
source ~/.bashrc
```

### 3. Spark 설치

Spark를 설치하는데 이번엔 이전 버전의 Spark를 설치한다. 만약 최신버전을 설치한다면

#### 1. 최신버전 설치
[Spark_Downloads](https://spark.apache.org/downloads.html)

![image.png](\assets\img\postimg\GCP\GCP_14.png)

원하는 옵션 선택 후 다운로드 링크를 복사하면 된다.

![image.png](\assets\img\postimg\GCP\GCP_15.png)

#### 2. 이전버전 설치

[Spark Archive](https://archive.apache.org/dist/spark/) 에 접속해 해당 버전의 tgz 파일의 링크를 복사한다.

---

이하 이전버전.
```bash
sudo wget https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz

sudo tar xvf spark-3.1.1-bin-hadoop2.7.tgz

sudo mkdir spark

sudo mv spark-3.1.1-bin-hadoop2.7/* /opt/spark/
```

**경로 확인**
```bash
cd $HOME

ls

pwd
```

![image.png](\assets\img\postimg\GCP\GCP_16.png)

`miniconda3` 가 있는 경로 확인. 나는 `/home/계정명`에 있다.

**vi 편집기 실행**
```bash
vi ~/.bashrc
```

**Spark 환경변수 설정**
```bash
# SPARK ENV SET
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=/home/경로/miniconda3/bin/python
```

**수정된 파일 실행**
```bash
source ~/.bashrc
```

![image.png](\assets\img\postimg\GCP\GCP_17.png)

위 과정을 모두 거치면 세 개의 환경변수 설정이 완료 되어있어야 한다.

### 4. PySpark 설치

**설치하기**
```bash
pip install pyspark==3.1.1
```
위에서 불러온 버전과 맞는 버전으로 설치하자.

**실행하기**
```bash
cd $HOME

pyspark
```

![image.png](\assets\img\postimg\GCP\GCP_18.png)

**Spark 설치 완료!!**

### 5. Jupyter Notebook 설치

**설치**

```bash
conda install jupyter notebook
```

**`config` 파일 생성**
```bash
jupyter notebook --generate-config
```

**해당 문장이 출력됨**
```bash
# Writing default config to: /home/kimec995/.jupyter/jupyter_notebook_config.py        
```

**경로 이동**
```bash
cd /home/계정명
```

**vi 이동**
```bash
vi ~/.jupyter/jupyter_notebook_config.py 
```

**vi 편집하기**

**찾기**
```bash
/ allow_root
```

**`True`로 변경**
![image.png](\assets\img\postimg\GCP\GCP_19.png)

**찾기**
```bash
/ NotebookApp.ip
```

**`0.0.0.0`으로 변경**
![image.png](\assets\img\postimg\GCP\GCP_20.png)

**Jupyter Notebook 실행**

```bash
jupyter notebook
```

**token 복사**

![image.png](\assets\img\postimg\GCP\GCP_21.png)
경로를 기억하자

ba2fe1bb8fdac2f9a5f41328731510546a59610952188dee

브라우저의 이동탭에 본인 인스턴스의 `외부IP`와 JupyterNotebook 에서 띄운 Port 번호를 입력하자.

![image.png](\assets\img\postimg\GCP\GCP_22.png)

token 입력하면 접속 완료!!

### 테스트

**버전확인**
```python
import pyspark
pyspark.__version__
```

**스파크 설치**
```python
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql import SparkSession

spark = (SparkSession.builder.appName("Authors").getOrCreate())
spark
```

링크의 Port 번호를 확인, 마찬가지로 `외부IP` + `SparkPort` 입력

![image.png](\assets\img\postimg\GCP\GCP_23.png)

GCP의 환경에 Jupyter lab과 Spark를 설치했다!!


## 마무리
사용 안하면 꼭!!!! 끄자!!!